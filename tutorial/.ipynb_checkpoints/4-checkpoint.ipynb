{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/Users/denoyer/workspace/rlstructures'))\n",
    "\n",
    "from rlstructures import Agent,DictTensor\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym.wrappers import TimeLimit\n",
    "from rlstructures.env_wrappers import NDiscreteGymEnv\n",
    "from rlstructures.batchers import EpisodeBatcher,Batcher\n",
    "\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# We redefine the previous environment. \n",
    "# As a little change, we allow one to provide a dictionary as an argument to the reset function\n",
    "# Each environment will be associated with an env_id specified by the user (for debugging for instance)\n",
    "# %%\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def seed(self,seed=None):\n",
    "        self.np_random,seed=seeding.np_random(seed)\n",
    "\n",
    "    def reset(self,env_info={\"env_id\":0}):\n",
    "        assert \"env_id\" in env_info        \n",
    "        self.env_id=env_info[\"env_id\"]\n",
    "        self.x=self.np_random.rand()*2.0-1.0\n",
    "        self.identifier=self.np_random.rand()\n",
    "        obs={\"x\":self.x,\"identifier\":self.identifier,\"env_id\":self.env_id}       \n",
    "        return obs\n",
    "\n",
    "    def step(self,action):\n",
    "        if action==0:\n",
    "            self.x-=0.3\n",
    "        else:\n",
    "            self.x+=0.3\n",
    "        done = self.x<-1 or self.x>1\n",
    "        \n",
    "        obs={\"x\":self.x,\"identifier\":self.identifier,\"env_id\":self.env_id},self.x,done,{}        \n",
    "        return obs\n",
    "\n",
    "# %% [markdown]\n",
    "# # Multithread Batchers\n",
    "# \n",
    "# RLStructures provides 2 different Multithread batchers. The precise explanation about how multithread batchers are working will be given in an other example. \n",
    " \n",
    "# 1) The EpisodeBatcher allows one to sample complete episodes. \n",
    "# 2) The Batcher allows to sample n timesteps only\n",
    "\n",
    "# Let us first illustrate the EpisodeBatcher.\n",
    "# Agent:\n",
    "# * For multithread batchers, the Agent has few differences; o) a buffer argument is added to the constructor ii) the ___call__ method has two additional arguments (slots, position_in_slots) which meaning will be given later. ii) the agent gives details about the information that it generates (specs) \n",
    "\n",
    "# Our specific agent is also taking as an input (throught agent_info) and agent_id to illustrate the use of agent_info\n",
    "# %%\n",
    "\n",
    "class UniformAgent(Agent):\n",
    "    def __init__(self,buffer,n_actions):\n",
    "        super().__init__(buffer=buffer)\n",
    "        self.n_actions=n_actions\n",
    "    \n",
    "    def __call__(self,state,observation,agent_info, slots,position_in_slots):\n",
    "        B=observation.n_elems()\n",
    "        agent_state=None\n",
    "        if state is None:\n",
    "            agent_state=DictTensor({\"timestep\":torch.zeros(B).long()})\n",
    "        else:\n",
    "            agent_state=state\n",
    "\n",
    "        scores=torch.randn(B,self.n_actions)\n",
    "        probabilities=torch.softmax(scores,dim=1)\n",
    "        actions=torch.distributions.Categorical(probabilities).sample()\n",
    "        new_state=DictTensor({\"timestep\":agent_state[\"timestep\"]+1})\n",
    "        # We also decide to output the action probabilities\n",
    "        return agent_state,DictTensor({\"action\":actions,\"action_probabilities\":probabilities,\"agent_id\":agent_info[\"agent_id\"]}),new_state\n",
    "\n",
    "    def specs_output(self):  \n",
    "        # Used to setup the buffer\n",
    "        specs={}      \n",
    "        specs[\"action\"] = {\"size\": torch.Size([]), \"dtype\": torch.int64}        \n",
    "        specs[\"agent_id\"] = {\"size\": torch.Size([]), \"dtype\": torch.int64}        \n",
    "        specs[\"action_probabilities\"] = {\"size\": torch.Size([self.n_actions]), \"dtype\": torch.float32}        \n",
    "        return specs\n",
    "\n",
    "    def specs_state(self):\n",
    "        # Used to setup the buffer\n",
    "        specs = {}\n",
    "        specs[\"timestep\"] = {\"size\": torch.Size([]), \"dtype\": torch.int64}\n",
    "        return specs\n",
    "\n",
    "# %% [markdown]\n",
    "# Now, let us declare functions to create the agent and the environment\n",
    "\n",
    "# %%\n",
    "\n",
    "def create_env(seed=0,max_episode_steps=100):\n",
    "    envs=[]\n",
    "    for k in range(4):\n",
    "        e=MyEnv()\n",
    "        e=TimeLimit(e, max_episode_steps=max_episode_steps)\n",
    "        envs.append(e)\n",
    "    return NDiscreteGymEnv(envs,seed=seed)\n",
    "\n",
    "def create_agent(buffer=None,n_actions=None):\n",
    "    # Here, the buffer argument must be specified\n",
    "    return UniformAgent(buffer,n_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating buffer for 'timestep' of size (128, 100) and type torch.int64\n",
      "[DEBUG] Creating buffer for 'action' of size (128, 100) and type torch.int64\n",
      "[DEBUG] Creating buffer for 'agent_id' of size (128, 100) and type torch.int64\n",
      "[DEBUG] Creating buffer for 'action_probabilities' of size (128, 100, 2) and type torch.float32\n",
      "[DEBUG] Creating buffer for 'x' of size (128, 100, 1) and type torch.float32\n",
      "[DEBUG] Creating buffer for 'identifier' of size (128, 100, 1) and type torch.float32\n",
      "[DEBUG] Creating buffer for 'env_id' of size (128, 100, 1) and type torch.float32\n",
      "[DEBUG] Creating buffer for 'reward' of size (128, 100) and type torch.float32\n",
      "[DEBUG] Creating buffer for 'done' of size (128, 100) and type torch.bool\n",
      "[DEBUG] Creating buffer for 'initial_state' of size (128, 100) and type torch.bool\n",
      "[DEBUG] Creating buffer for 'last_action' of size (128, 100) and type torch.int64\n",
      "[DEBUG] Creating buffer for '_timestep' of size (128, 100) and type torch.int64\n",
      "[DEBUG] Creating buffer for '_x' of size (128, 100, 1) and type torch.float32\n",
      "[DEBUG] Creating buffer for '_identifier' of size (128, 100, 1) and type torch.float32\n",
      "[DEBUG] Creating buffer for '_env_id' of size (128, 100, 1) and type torch.float32\n",
      "[DEBUG] Creating buffer for '_reward' of size (128, 100) and type torch.float32\n",
      "[DEBUG] Creating buffer for '_done' of size (128, 100) and type torch.bool\n",
      "[DEBUG] Creating buffer for '_initial_state' of size (128, 100) and type torch.bool\n",
      "[DEBUG] Creating buffer for '_last_action' of size (128, 100) and type torch.int64\n",
      "[DEBUG] Creating buffer for 'position_in_slot' of size (128, 100) and type torch.int64\n",
      "[INFO] [EpisodeBatcher] Creating 4 threads\n",
      "Starting the acquisition process\n"
     ]
    }
   ],
   "source": [
    "    import torch.multiprocessing as mp\n",
    "\n",
    "    mp.set_start_method(\"spawn\")    \n",
    "    batcher=EpisodeBatcher(\n",
    "            n_timesteps=100,\n",
    "            n_slots=128,\n",
    "            n_threads=4,\n",
    "            seeds=[1,2,3,4],        \n",
    "            create_agent=create_agent,\n",
    "            agent_args={\"n_actions\":2},\n",
    "            create_env=create_env,\n",
    "            env_args={\"max_episode_steps\":100}\n",
    "    )\n",
    "    #We want to sample 32 episodes (Note that since our batcher has 4 threads with 4 envs per thread, we must sample 16,32 , 48, ... episodes)    \n",
    "    print(\"Starting the acquisition process\")\n",
    "    n_episodes=32\n",
    "\n",
    "    #Information to pass to the 32 agents, and 32 environments\n",
    "    agent_info=DictTensor({\"agent_id\":torch.arange(32)})    \n",
    "    env_info=DictTensor({\"env_id\":torch.arange(32)})    \n",
    "    \n",
    "    #Running the batcher. It is a non-blocking function that launch the acqusition\n",
    "    batcher.execute(n_episodes=32,agent_info=agent_info,env_info=env_info)\n",
    "    #Getting episodes -- not that get is a blocking function such that the process will wait until the end of the acquisition. \n",
    "    #The non-blocking variant can be used as get(blocking=False) returning None if the acquisition process is not finished\n",
    "    trajectories=batcher.get()\n",
    "\n",
    "    idx=5\n",
    "    print(\"Lengths of trajectories = \",trajectories.lengths)\n",
    "    print(\"For trajectory #\"+str(idx))\n",
    "    print(\"\\th_t: \",trajectories[\"timestep\"][idx])\n",
    "    print(\"\\tz_t: \",trajectories[\"_timestep\"][idx])\n",
    "    print(\"\\tagent_id: \",trajectories[\"agent_id\"][idx])\n",
    "    print(\"\\tenv_id: \",trajectories[\"env_id\"][idx].squeeze(-1))\n",
    "                                                                      \n",
    "    print(\"\\tReward received after each action \",trajectories[\"_reward\"][idx])\n",
    "    print(\"\\tAction at each timestep \",trajectories[\"action\"][idx])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
