Implemeting Recurrent Policies
=============================

We explain how to implement recurrent policies. We need first a reccurent model for the policy and the critic:

.. code-block:: python

    class AgentModel(nn.Module):
        """ The model that computes one score per action
        """
        def __init__(self, n_observations, n_actions, n_hidden):
            super().__init__()
            self.linear = nn.Linear(n_observations, n_hidden)
            self.linear_state = nn.Linear(n_hidden, n_hidden)
            self.linear_z = nn.Linear(n_hidden*2, n_hidden)

            self.linear2 = nn.Linear(n_hidden, n_actions)
            self.n_hidden=n_hidden

        def initial_state(self,B):
            return torch.zeros(B,self.n_hidden)

        def forward(self, state,frame):
            frame = torch.tanh(self.linear(frame))
            state=torch.tanh(self.linear_state(state))
            z=torch.tanh(self.linear_z(torch.cat([frame,state],dim=1)))
            score_actions = self.linear2(z)
            probabilities_actions = torch.softmax(score_actions,dim=-1)
            return z,probabilities_actions

    class BaselineModel(nn.Module):
        """ The model that computes V(s)
        """
        def __init__(self, n_observations, n_hidden):
            super().__init__()
            self.linear = nn.Linear(n_observations, n_hidden)
            self.linear_state = nn.Linear(n_hidden, n_hidden)
            self.linear_z = nn.Linear(n_hidden*2, n_hidden)
            self.linear2 = nn.Linear(n_hidden, 1)


        def forward(self,state, frame):
            frame = torch.tanh(self.linear(frame))
            state=torch.tanh(self.linear_state(state))
            z=torch.tanh(self.linear_z(torch.cat([frame,state],dim=1)))
            critic = self.linear2(z)
            return z,critic

On top of that, we will adapt our Agent as follows:

.. code-block:: python

    class RecurrentAgent(Agent):
        def __init__(self,model=None, n_actions=None):
            super().__init__()
            self.model = model
            self.n_actions = n_actions


        def update(self,  state_dict):
            self.model.load_state_dict(state_dict)

        def __call__(self, state, observation,agent_info=None,history=None):
            """
            Executing one step of the agent
            """
            # Verify that the batch size is 1
            initial_state = observation["initial_state"]
            B = observation.n_elems()

            if agent_info is None:
                agent_info=DictTensor({"stochastic":torch.tensor([True]).repeat(B)})

            # Create the initial state of the recurrent policy
            agent_initial=self.model.initial_state(B)
            if (state is None): # If the batcher is starting
                state=DictTensor({"agent_state":agent_initial,"agent_step":torch.zeros(B).long()})
            else:
                #Maybe some observations are initial states of new episodes. For these state, we must initialize the internal state of the policy
                istate=DictTensor({"agent_state":agent_initial,"agent_step":torch.zeros(B).long()})
                state=masked_dicttensor(istate,state,initial_state)


            new_z,action_proba = self.model(state["agent_state"],observation["frame"])

            #We sample an action following the distribution
            dist = torch.distributions.Categorical(action_proba)
            action_sampled = dist.sample()

            #Depending on the agent_info variable that tells us if we are in 'stochastic' or 'deterministic' mode, we keep the sampled action, or compute the action with the max score
            action_max = action_proba.max(1)[1]
            smask=agent_info["stochastic"].float()
            action=masked_tensor(action_max,action_sampled,agent_info["stochastic"])


            new_state = DictTensor({"agent_state":new_z,"agent_step": state["agent_step"] + 1})

            agent_do = DictTensor(
                {"action": action, "action_probabilities": action_proba}
            )

            return state, agent_do, new_state

Adapting the loss function
--------------------------

The A2C loss function needs to be adapted to this particular agent in the way the action probabilities and critic values are computed:

.. code-block:: python

            action_probabilities=[]
            agent_state=trajectories["agent_state"][:,0]
            for t in range(max_length):
                agent_state,proba=self.learning_model(agent_state,trajectories["frame"][:,t])
                action_probabilities.append(proba.unsqueeze(1)) # We append the probability, and introduces the temporal dimension (2nde dimension)
            action_probabilities=torch.cat(action_probabilities,dim=1) #Now, we have a B x T x n_actions tensor

            #We compute the critic value for t=0 to T (i.e including the very last observation)
            critic=[]
            agent_state=trajectories["agent_state"][:,0]
            for t in range(max_length):
                agent_state,b=self.critic_model(agent_state,trajectories["frame"][:,t])
                critic.append(b.unsqueeze(1))
            critic=torch.cat(critic+[b.unsqueeze(1)],dim=1).squeeze(-1) #Now, we have a B x (T+1) tensor
            #We also need to compute the critic value at for the last observation of the trajectories (to compute the TD)
            # It may be the last element of the trajectories (if episode is not finished), or on the last frame of the episode
            idx=torch.arange(trajectories.n_elems())
            _,last_critic=self.critic_model(trajectories["_agent_state"][idx,trajectories.lengths-1],trajectories["_frame"][idx,trajectories.lengths-1])
            last_critic=last_critic.squeeze(-1)
            critic[idx,trajectories.lengths]=last_critic
